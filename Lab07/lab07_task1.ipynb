{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"lab07_task1.ipynb","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"},"language_info":{"name":"python"}},"cells":[{"cell_type":"code","metadata":{"id":"ltnC-DbVFNrl","executionInfo":{"status":"ok","timestamp":1630611071279,"user_tz":-330,"elapsed":686,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["import nltk\n","from nltk.corpus import twitter_samples \n","import pandas as pd\n","from sklearn.model_selection import train_test_split\n","import tensorflow.compat.v1 as tf\n","tf.disable_v2_behavior()"],"execution_count":24,"outputs":[]},{"cell_type":"code","metadata":{"id":"nEw35lgbIKmJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630611072419,"user_tz":-330,"elapsed":18,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}},"outputId":"d3851e37-5578-43e4-e92d-3bc09073ce99"},"source":["nltk.download('twitter_samples')\n","nltk.download('stopwords')"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["[nltk_data] Downloading package twitter_samples to /root/nltk_data...\n","[nltk_data]   Package twitter_samples is already up-to-date!\n","[nltk_data] Downloading package stopwords to /root/nltk_data...\n","[nltk_data]   Package stopwords is already up-to-date!\n"]},{"output_type":"execute_result","data":{"text/plain":["True"]},"metadata":{},"execution_count":25}]},{"cell_type":"code","metadata":{"id":"K8zAwNf3IcwC","executionInfo":{"status":"ok","timestamp":1630611072421,"user_tz":-330,"elapsed":12,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["import re\n","import string\n","import numpy as np\n","\n","from nltk.corpus import stopwords\n","from nltk.stem import PorterStemmer\n","from nltk.tokenize import TweetTokenizer"],"execution_count":26,"outputs":[]},{"cell_type":"code","metadata":{"id":"VOkRSbEw1Shr","executionInfo":{"status":"ok","timestamp":1630611072422,"user_tz":-330,"elapsed":12,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["#process_tweet(): cleans the text, tokenizes it into separate words, removes stopwords, and converts words to stems.\n","def process_tweet( tweet ):\n","   \n","    stemmer = PorterStemmer()\n","    stopwords_english = stopwords.words('english')\n","\n","    # remove stock market tickers like $GE\n","    tweet = re.sub( r'\\$\\w*', '', tweet )\n","\n","    # remove old style retweet text \"RT\"\n","    tweet = re.sub( r'^RT[\\s]+', '', tweet )\n","    \n","    # remove hyperlinks\n","    tweet = re.sub( r'https?:\\/\\/.*[\\r\\n]*', '', tweet )\n","    \n","    # remove hashtags\n","    # only removing the hash # sign from the word\n","    tweet = re.sub( r'#', '', tweet )\n","    \n","    # tokenize tweets\n","\n","    tokenizer = TweetTokenizer( preserve_case = False, strip_handles = True, reduce_len = True )\n","    tweet_tokens = tokenizer.tokenize(tweet)\n","\n","    tweets_clean = []\n","    for word in tweet_tokens:  \n","        #############################################################\n","        # 1 remove stopwords\n","        # 2 remove punctuation\n","        # 3 stemming word\n","        # 4 Add it to tweets_clean\n","        if(word not in stopwords_english and word not in string.punctuation):\n","            stem_word = stemmer.stem(word)\n","            tweets_clean.append(stem_word)\n","    return tweets_clean"],"execution_count":27,"outputs":[]},{"cell_type":"code","metadata":{"id":"f6v_JLxe1ZXh","executionInfo":{"status":"ok","timestamp":1630611072423,"user_tz":-330,"elapsed":11,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["#build_freqs counts how often a word in the 'corpus' (the entire set of tweets) was associated with\n","  # a positive label '1'         or \n","  # a negative label '0', \n","\n","#then builds the freqs dictionary, where each key is a (word,label) tuple, \n","\n","#and the value is the count of its frequency within the corpus of tweets.\n","\n","def build_freqs( tweets, ys ):\n","    \n","    # Convert np array to list since zip needs an iterable.\n","    # The squeeze is necessary or the list ends up with one element.\n","    # Also note that this is just a NOP if ys is already a list.\n","    yslist = np.squeeze(ys).tolist()\n","\n","    # Start with an empty dictionary and populate it by looping over all tweets\n","    # and over all processed words in each tweet.\n","    freqs = {}\n","\n","    for y, tweet in zip( yslist, tweets ):\n","        for word in process_tweet(tweet):\n","            pair = ( word, y ) \n","            #############################################################\n","            #Update the count of pair if present, set it to 1 otherwise\n","            if pair in freqs:\n","              freqs[pair] += 1\n","            else:\n","              freqs[pair] = 1\n","\n","    return freqs"],"execution_count":28,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"zG07lgkRFNtF"},"source":["### Prepare the data\n","* The `twitter_samples` contains subsets of 5,000 positive tweets, 5,000 negative tweets, and the full set of 10,000 tweets.  "]},{"cell_type":"code","metadata":{"id":"Mt_btshaFNtJ","executionInfo":{"status":"ok","timestamp":1630611073124,"user_tz":-330,"elapsed":711,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["# select the set of positive and negative tweets\n","all_positive_tweets = twitter_samples.strings('positive_tweets.json')\n","all_negative_tweets = twitter_samples.strings('negative_tweets.json')"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"bFUFtkzCFNt7"},"source":["* Train test split: 20% will be in the test set, and 80% in the training set.\n"]},{"cell_type":"code","metadata":{"id":"PNp1zFFmFNuA","executionInfo":{"status":"ok","timestamp":1630611073125,"user_tz":-330,"elapsed":10,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["# split the data into two pieces, one for training and one for testing\n","#############################################################\n","test_pos = all_positive_tweets[4000:]\n","train_pos = all_positive_tweets[:4000]\n","test_neg = all_negative_tweets[4000:]\n","train_neg = all_negative_tweets[:4000]\n","\n","train_x = train_pos + train_neg\n","test_x = test_pos + test_neg"],"execution_count":30,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JyC4UAjZFNuT"},"source":["* Create the numpy array of positive labels and negative labels."]},{"cell_type":"code","metadata":{"id":"xS1wvGq7FNuX","executionInfo":{"status":"ok","timestamp":1630611073126,"user_tz":-330,"elapsed":10,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["# combine positive and negative labels\n","train_y = np.append( np.ones(( len(train_pos), 1 )), np.zeros(( len(train_neg), 1 )), axis = 0 )\n","test_y = np.append( np.ones(( len(test_pos), 1 )), np.zeros(( len(test_neg), 1 )), axis = 0 )"],"execution_count":31,"outputs":[]},{"cell_type":"code","metadata":{"id":"SOikeLT0DelO","executionInfo":{"status":"ok","timestamp":1630611073127,"user_tz":-330,"elapsed":10,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["Final_data = all_positive_tweets + all_negative_tweets\n","data = np.append( np.ones(( len(all_positive_tweets), 1 )), np.zeros(( len(all_negative_tweets), 1 )), axis = 0 )\n","train_x, test_x, train_y, test_y = train_test_split( Final_data, data, test_size = 0.20, random_state = 67 )"],"execution_count":32,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"RvppFUyLFNu2"},"source":["* Create the frequency dictionary using the  `build_freqs()` function.  \n","    \n"]},{"cell_type":"code","metadata":{"id":"ggrsgUh4FNu5","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1630611076218,"user_tz":-330,"elapsed":3100,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}},"outputId":"50813748-5e3b-4573-a96b-41182763bcc1"},"source":["# create frequency dictionary\n","#############################################################\n","freqs = build_freqs( train_x, train_y )\n","\n","# check the output\n","print(\"type(freqs) = \" + str(type(freqs)))\n","print(\"len(freqs) = \" + str(len(freqs.keys())))"],"execution_count":33,"outputs":[{"output_type":"stream","name":"stdout","text":["type(freqs) = <class 'dict'>\n","len(freqs) = 11309\n"]}]},{"cell_type":"markdown","metadata":{"id":"-pMQQEM8FNxr"},"source":["## Extracting the features\n","\n","* Given a list of tweets, extract the features and store them in a matrix. You will extract two features.\n","    * The first feature is the number of positive words in a tweet.\n","    * The second feature is the number of negative words in a tweet. \n","* Then train your logistic regression classifier on these features.\n","* Test the classifier on a validation set. \n"]},{"cell_type":"code","metadata":{"id":"tBqwBhBoFNx0","executionInfo":{"status":"ok","timestamp":1630611076220,"user_tz":-330,"elapsed":16,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["def extract_features( tweet, freqs ):\n","    \n","    # tokenizes, stems, and removes stopwords\n","    #############################################################\n","    word_l = process_tweet(tweet)\n","    \n","    # 2 elements in the form of a 1 x 2 vector\n","    x = np.zeros(( 1, 2 )) \n","     \n","        \n","    # loop through each word in the list of words\n","    for word in word_l:\n","        # increment the word count for the positive label 1\n","        if(( word, 1 ) in freqs):\n","          x[ 0, 0 ] += freqs[ word, 1 ]\n","        # increment the word count for the negative label 0\n","        if(( word, 0 ) in freqs):\n","          x[ 0, 1 ] += freqs[ word, 0 ]\n","    \n","    assert( x.shape == ( 1, 2 ) )\n","    return x[0]"],"execution_count":34,"outputs":[]},{"cell_type":"code","metadata":{"id":"9Zac7-l0FluN","executionInfo":{"status":"ok","timestamp":1630611076223,"user_tz":-330,"elapsed":17,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["#pred function\n","def predict_tweet( tweet ):\n","    with tf.Session() as sess:\n","      saver.restore( sess, save_path = 'TSession' )\n","      data_i = []\n","      for t in tweet:\n","        data_i.append( extract_features( t, freqs ) )\n","      data_i = np.asarray( data_i )\n","      return sess.run( tf.nn.sigmoid( tf.add( tf.matmul( a = data_i, b = W, transpose_b = True ), bias ) ) )\n","    print(\"--Fail--\")\n","    return"],"execution_count":35,"outputs":[]},{"cell_type":"code","metadata":{"id":"DTvbyH68FpcJ","executionInfo":{"status":"ok","timestamp":1630611076225,"user_tz":-330,"elapsed":17,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["bias = tf.Variable( np.random.randn(1), name = \"Bias\" )\n","W = tf.Variable( np.random.randn( 1, 2 ), name = \"Weight\" )"],"execution_count":36,"outputs":[]},{"cell_type":"code","metadata":{"id":"gVMABY8pFvcp","executionInfo":{"status":"ok","timestamp":1630611079859,"user_tz":-330,"elapsed":3648,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["data = []\n","for t in train_x:\n","  data.append( extract_features( t, freqs ) )\n","data = np.asarray( data )"],"execution_count":37,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"vqA64f_QFy8g","executionInfo":{"status":"ok","timestamp":1630611079860,"user_tz":-330,"elapsed":14,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}},"outputId":"9309994c-dd4b-4ab7-e468-a38dd579bd89"},"source":["Y_hat = tf.nn.sigmoid( tf.add( tf.matmul( np.asarray(data), W, transpose_b = True), bias ) ) \n","ta = np.asarray( train_y )\n","Total_cost = tf.nn.sigmoid_cross_entropy_with_logits( logits = Y_hat, labels = ta ) \n","\n","print(Total_cost)"],"execution_count":38,"outputs":[{"output_type":"stream","name":"stdout","text":["Tensor(\"logistic_loss_1:0\", shape=(8000, 1), dtype=float64)\n"]}]},{"cell_type":"code","metadata":{"id":"8ikQdvruGO0P","executionInfo":{"status":"ok","timestamp":1630611079863,"user_tz":-330,"elapsed":11,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["# Gradient Descent Optimizer \n","optimizer = tf.train.GradientDescentOptimizer( learning_rate = 0.00001 ,name = \"GradientDescent\" ).minimize( Total_cost ) \n","\n","# Global Variables Initializer \n","init = tf.global_variables_initializer()"],"execution_count":39,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"GjDZ0LkAGQMF","executionInfo":{"status":"ok","timestamp":1630611097734,"user_tz":-330,"elapsed":17880,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}},"outputId":"365861b8-f5d5-412d-b911-6ef637fe5fd2"},"source":["saver = tf.train.Saver()\n","\n","with tf.Session() as sess:  \n","  sess.run( init )\n","  \n","  print( \"Bias\", sess.run( bias ) )\n","  print( \"Weight\", sess.run( W ) )\n","  \n","  for epoch in range(1000):\n","    sess.run( optimizer )\n","    preds = sess.run( Y_hat )\n","    acc=( ( preds == ta ).sum() )/len(train_y)\n","    Accuracy = []\n","    repoch = False\n","    if repoch:\n","      Accuracy.append( acc )\n","    if epoch % 1000 == 0:\n","      print( \"Accuracy = \", acc )\n","    saved_path = saver.save( sess, 'TSession' )"],"execution_count":40,"outputs":[{"output_type":"stream","name":"stdout","text":["Bias [0.19067353]\n","Weight [[-0.60400689 -0.25248003]]\n","Accuracy =  0.443375\n"]}]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"6dqh-5RyGW2C","executionInfo":{"status":"ok","timestamp":1630611098483,"user_tz":-330,"elapsed":787,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}},"outputId":"27f34fda-5add-4e3f-8942-02565836ae52"},"source":["preds = predict_tweet( test_x )\n","print( preds, len(test_y) )"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["INFO:tensorflow:Restoring parameters from TSession\n","[[5.65643231e-258]\n"," [0.00000000e+000]\n"," [0.00000000e+000]\n"," ...\n"," [0.00000000e+000]\n"," [0.00000000e+000]\n"," [0.00000000e+000]] 2000\n"]}]},{"cell_type":"code","metadata":{"id":"2Fh_gJ59GcN8","executionInfo":{"status":"ok","timestamp":1630611098484,"user_tz":-330,"elapsed":24,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":["def accuracy( x, y ):\n","  return ( ( x == y ).sum() )/len(y)\n"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"jvbnT6oWGgyM","executionInfo":{"status":"ok","timestamp":1630611098486,"user_tz":-330,"elapsed":25,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}},"outputId":"e4932c0e-2d27-4a89-9b03-1ee37d82d248"},"source":["print( accuracy( preds, test_y ) )"],"execution_count":43,"outputs":[{"output_type":"stream","name":"stdout","text":["0.4355\n"]}]},{"cell_type":"code","metadata":{"id":"SAsSyT-xGk13","executionInfo":{"status":"ok","timestamp":1630611098487,"user_tz":-330,"elapsed":21,"user":{"displayName":"CE067_Deep_Lad","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14Gjlsw0W0nD62k2RbPqKceq0gVWeMlAILz9Y-N7R=s64","userId":"11647310477931572941"}}},"source":[""],"execution_count":43,"outputs":[]}]}